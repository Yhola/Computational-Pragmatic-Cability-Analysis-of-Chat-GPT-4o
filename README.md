# Computational-Pragmatic-Cability-Analysis-of-Chat-GPT-4o
Overview
This repository contains two Python scripts related to analyzing AI-generated academic writing quality using a set of predefined checklist items. The research focuses on ChatGPT 4.0 outputs and evaluates paragraphs against 24 factors that measure both contextual and linguistic control elements in academic writing.

First Analysis Code:
This script:

Reads two datasets from Word documents (Dataset 1 and Dataset 2).
Each dataset contains 12 paragraphs separated by ####.
Assigns binary scores (0 or 1) to each paragraph based on 24 factors.
Produces a table with paragraph scores for each dataset and shows the total score out of 24 for each paragraph.
Factor-wise Analysis Code:
This script:

Uses the same datasets and scoring functions.
Aggregates scores for each checklist item across all paragraphs.
Produces a table with each checklist item’s total score from Dataset 1 and Dataset 2.
Identifies which factors achieved the highest and lowest scores.
Highlights the performance differences between datasets for individual factors.
Key Features
Automatic Parsing of Paragraphs:
The scripts use python-docx to read text from Word documents and split paragraphs based on the #### delimiter.

Binary Scoring of 24 Factors:
Each paragraph is analyzed against 24 factors. If a factor is present, the script assigns a score of 1; if absent, it assigns 0. The factors cover a range of contextual and linguistic dimensions required for critical academic writing at MSc level.

Aggregation and Comparison of Scores:
The first analysis script generates paragraph-level results. The second script provides factor-level totals, enabling a clearer understanding of where the model performs well or struggles.

Simple, Adaptable Code:
The code uses straightforward keyword-based searches for factors. Although this approach is basic, it can be modified or improved depending on the researcher’s needs.

Requirements
Python 3.x
python-docx
pandas
re (regular expressions, included in the standard library)
You can install the required packages with:

bash
Copy code
pip install python-docx pandas
Usage
Prepare the Data:

Convert your Word files into .docx format if they are not already.
Make sure your datasets (Prompt Dataset.docx and Cognitive Dataset.docx) are placed in the same directory as the scripts.
Each .docx file should contain 12 paragraphs separated by a line containing ####.
Run the First Analysis Code:

bash
Copy code
python first_analysis_code.py
This will:

Prompt you to specify the dataset file paths (if not already hardcoded).
Perform the paragraph-level scoring.
Print a table showing paragraph scores for both datasets.
Run the Factor-wise Analysis Code:

bash
Copy code
python factor_wise_analysis_code.py
This will:

Use the same dataset files.
Produce a table with each factor’s total score for Dataset 1 and Dataset 2.
Display the highest and lowest scoring factors for each dataset.
Interpreting the Results
Paragraph-level Results:
Use the table generated by the first analysis to see how each paragraph scored out of 24. Higher scores indicate that the paragraph included more of the desired factors.

Factor-wise Results:
Check the factor-level table from the second script to understand which factors the model readily incorporated and which it struggled with. This can guide prompt adjustments or further training data curation.

Limitations
The factor detection relies on simple keyword matching. It may not fully capture the depth of pragmatic and contextual elements intended in the criteria.
Some factors may score zero if not explicitly represented by the chosen keywords.
Contributing
You can improve factor detection methods, add more sophisticated NLP techniques, or integrate human-in-the-loop evaluations. Pull requests and suggestions are welcome.
